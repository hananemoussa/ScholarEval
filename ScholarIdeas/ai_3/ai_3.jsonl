{"statement": "The proposed SWR method is computationally more efficient than previously proposed methods for addressing plasticity loss.", "type": "strength", "axis": "contribution", "severity": "minor"}
{"statement": "The metric defined as the ratio between the Frobenius norm of the current weight matrix and that of the initial one may not well capture the extent of change in the model, as applying weight regularization could significantly alter the weights yet the model's performance may change only marginally.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The main novelty of the SWR idea is limited and comes primarily from scaling the bias vectors according to a certain rule.", "type": "weakness", "axis": "contribution", "severity": "major"}
{"statement": "Missing baselines including Lyle et al. 2024 which showed that L2 + Layer norm generally outperforms the majority of existing methods, and Lee et al. 2024 which showed superior generalization performance on these benchmarks.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The claimed computational overhead cost of regularization methods like L2 is not significantly high unless higher-order computation is involved, and L2 is quite common even in large-scale models.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "One main drawback is the limited application of the plan due to many assumptions (e.g., the network is affine, homogeneous with ReLU), which impedes the contributions and applicability in real-world scenarios.", "type": "weakness", "axis": "contribution", "severity": "major"}
{"statement": "There are no experiments that consider the effect of the hyperparameter \u03b1 on the boundedness of the weight before and after scaling in the SWR method.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The set of competitor algorithms is limited, specifically missing well-cited re-initialization based methods like Continual Backprop (Dohare et al.) and ReDO (Sokar et al.) for plasticity loss evaluation, and regularization based methods like Elastic Weight Consolidation (Kirkpatrick et al.) for catastrophic forgetting evaluation.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "To eventually establish that SWR mitigates catastrophic forgetting requires more evidence than a single experiment, and the limited experiments do not provide sufficient evidence that catastrophic forgetting is alleviated by SWR more efficiently than by other algorithms.", "type": "weakness", "axis": "soundness", "severity": "major"}
