{"statement": "No experiments are conducted to quantify the sim-to-real benefits derived from using a real-world city layout based on Beijing's commercial district, leaving the practical advantages of this choice unclear.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The benchmark predominantly focuses on drone-related tasks, with limited discussion on tasks relevant to autonomous vehicle planning.", "type": "weakness", "axis": "contribution", "severity": "minor"}
{"statement": "The tasks are largely oriented toward language-based interactions, with an emphasis on using large language models.", "type": "weakness", "axis": "contribution", "severity": "minor"}
{"statement": "Metrics like BLEU and ROUGE, which primarily measure text quality, may not fully capture the performance of embodied AI tasks, raising questions about the suitability of these metrics for this benchmark.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The diversity of the QA templates appears to be quite limited, as only three categories are mentioned: distance questions, position questions, and counting questions.", "type": "weakness", "axis": "contribution", "severity": "minor"}
{"statement": "There appears to be no strong connection between the proposed tasks and the roles of pedestrians and vehicles, which raises questions about their necessity in the framework.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The usefulness of the proposed benchmark is not adequately established due to the absence of learnable baselines to validate the dataset's rationale.", "type": "weakness", "axis": "contribution", "severity": "major"}
{"statement": "The authors have not conducted experiments to explore the impact of different embodiments on task performance, which could provide valuable insights into the effectiveness of various embodiment strategies.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "Evaluating Embodied QA, Embodied Dialogue, and Embodied Task Planning with captioning and translation metrics like BLEU and CIDEr seems like a poor choice for measuring task success.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The research plan lacks clarity on how much navigation, if any, is required for the Embodied QA, Embodied Dialogue, and Embodied Task Planning tasks.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The motivation behind this research aligns with the principles of ELM, focusing on embodied understanding in driving scenarios, but a detailed explanation of the differences between the two approaches is necessary.", "type": "weakness", "axis": "contribution", "severity": "minor"}
{"statement": "Most of the evaluation tasks already exist in current literature, and providing a detailed explanation to distinguish these tasks from those in other works is important.", "type": "weakness", "axis": "contribution", "severity": "major"}
