{"statement": "Improving stability of GAN training by increasing robustness properties via losses more robust to outliers and with bounded gradients is an interesting idea worth investigating.", "type": "strength", "axis": "contribution", "severity": "minor"}
{"statement": "The proposed zephyr loss function L(a) = \u03b1(\u221a(a\u00b2 + \u03b5) - \u221a\u03b5) looks very close to a Huber loss, but this connection is neither referenced nor discussed in the research plan.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The method appears to be most likely a specific instance of an f-gan and would benefit from analysis under this framework, but no references to f-gan related papers are included.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "Even if the zephyr loss function is convex, smooth, and Lipschitz continuous, this does not ensure that the training process of GAN is Lipschitz continuous and dynamically stable.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The research plan lacks analysis on how zephyr loss can stabilize GAN training and ensure that ZGAN meets Lipschitz continuity.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "No explanation is provided for why this loss function is called zephyr loss or whether there is any prior research on this function.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The planned experiments do not include analysis of ZGAN on high-resolution datasets such as LSUN, CeleBA, or ImageNet.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The research plan does not provide detailed ablation study methodology for the parameters \u03b1 and \u03b5 beyond mentioning systematic exploration of their values.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "All planned experiments use relatively simple networks and datasets, and validation would be strengthened by employing the proposed loss function with more advanced architectures like StyleGAN2 and StyleGAN-XL on challenging datasets such as FFHQ and AFHQ.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "Since the research focuses on stable training of GANs, experiments with small datasets should be conducted to verify whether the proposed loss function can stabilize GAN training on limited training data.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The planned baseline comparisons only include WGAN and LSGAN, but more comparisons with WGAN-GP and WGAN-LP are expected.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "Since zephyr loss is equivalent to estimating the Total Variation distance and Total Variation is not continuously differentiable with respect to the parameters as mentioned in WGAN, this is not ideal.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The planned experiments need to be conducted with modern implementations using AdamW (beta1=0), EMA, and modern methods, with baselines taken from their respective papers or re-implemented with modern methods.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "Making a new loss function is unlikely to solve the big problem with GANs which is stability when scaling.", "type": "weakness", "axis": "contribution", "severity": "major"}
