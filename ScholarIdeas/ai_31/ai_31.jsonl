{"statement": "This work largely builds on top of Denison et. al 2024 - using the same prompts, training/evaluation settings, etc. - and only additionally allows the models to re-attempt in-context (i.e. ICRL).", "type": "weakness", "axis": "contribution", "severity": "major"}
{"statement": "The core research question and motivation appear to be directly derived from SEG, raising concerns about the work's independent contribution to the field.", "type": "weakness", "axis": "contribution", "severity": "major"}
{"statement": "Models can hack rewards in-context, which is not surprising since in these environments, we are explicitly rewarding the model if they hack the reward.", "type": "weakness", "axis": "contribution", "severity": "minor"}
{"statement": "The approach of normalizing compute by using a constant number of output tokens may be inadequate since using longer contexts in ICRL should incur higher computational cost, and normalizing by FLOPS might be a better metric.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The task categories examined (Philosophical Sycophancy, Tool-Use Flattery, Nudged Rubric Modification, Insubordinate Rubric Modification, and Reward Tampering) are overly restrictive and fail to demonstrate applicability.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The authors only perform evaluations on models by OpenAI (gpt-4o, gpt-4o-mini, o1-preview, and o1-mini), which means that the generality of the results with regards to ICRL and reward hacking is not established.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The evaluation method for certain tasks might not reliably indicate specification gaming, as passing the task does not necessarily mean specification gaming has happened.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "Using success rate alone does not reflect the models internal 'motives' when evaluating whether the model genuinely agrees with the user or is performing reward hacking/sycophancy.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "This work assumes that all tasks have well-defined numeric rewards which can be unrealistic in most use cases of language models.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The term ICRL can be a bit of a misnomer since in traditional RL, the reward is only used for training the model and not actually shown during inference, and 'iterative refinement' might be more aligned with the research works cited.", "type": "weakness", "axis": "soundness", "severity": "minor"}
