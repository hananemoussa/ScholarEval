{"statement": "The authors should evaluate potential data leakage between training and test sets, as similar images with different sub-charts might appear across different samples from the same scientific domain.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The entire benchmark only consists of permutations (e.g., matching subfigures to subcaptions in the Figure Caption Matching tasks), which could bias pre-trained models toward memorizing associations instead of correctly perceiving details in the image and retrieving the relevant knowledge correctly.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The QA tasks focus on figure-caption matching which may not require fine-grained domain knowledge to analyze details within a figure but differences across figures, and might prioritize memory over understanding.", "type": "weakness", "axis": "contribution", "severity": "major"}
{"statement": "The human baseline using computer science graduate students to answer challenging natural science questions is not well designed, as it requires knowledge incompatible with computer science, making it unclear what accuracy a model should achieve to match a human expert with domain-specific knowledge.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "The tasks are somewhat standard - Figure captioning and matching figures/sub-figures to appropriate captions - and it would have been nice to see some unique tasks created from this dataset showcasing the diversity of images/plots, such as interleaved image-text Question Answering tasks.", "type": "weakness", "axis": "contribution", "severity": "minor"}
{"statement": "Some metrics for the Scientific Figure Captioning task are missing, such as BLEU, CIDEr, SPICE which are often used in figure captioning evaluations, and ROUGE alone is primarily a recall-based metric that is not sufficient for captioning evaluation.", "type": "weakness", "axis": "soundness", "severity": "minor"}
{"statement": "The MMSCICAP Scientific Figure Captioning task seems flawed by providing only the abstract rather than figure-specific sentences or the entire relevant section, which may undermine the task's grounding since abstracts alone should suffice only for captioning teaser figures.", "type": "weakness", "axis": "soundness", "severity": "major"}
{"statement": "Given the depth of scientific tasks in this graduate-level multimodal scientific understanding dataset, testing models like GPT-o1 or LMMs with enhanced reasoning abilities could yield crucial insights for LMM development.", "type": "weakness", "axis": "soundness", "severity": "minor"}
